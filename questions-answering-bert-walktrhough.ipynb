{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION-ANSWERING BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal of this notebook is to explore and understand pre-trained qa models from the huggingface library. No fine-tuning is going to be done, we are going to use existing models from **<a href=\"https://huggingface.co/models\">Hugging Face Models.</a>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BERT Input format for QA\n",
    "\n",
    "- To feed a QA task into BERT, we pack both the question and the reference text into the input.\n",
    "\n",
    "- The two pieces of text are separated by the special [SEP] token.\n",
    "- BERT also uses “Segment Embeddings” to differentiate the question from the reference text. These are simply two embeddings (for segments “A” and “B”) that BERT learned, and which it adds to the token embeddings before feeding them into the input layer.\n",
    "\n",
    "<img src=\"img/bert_qa_input.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 STAR & END TOKEN CLASSIFIER\n",
    "\n",
    "- BERT needs to highlight a “span” of text containing the answer–this is represented as simply predicting which token marks the start of the answer, and which token marks the end\n",
    "\n",
    "<img src=\"img/qa-bert-input-start.png\"> \n",
    "\n",
    "- For every token in the text, we feed its final embedding into the start token classifier. The start token classifier only has a single set of weights (represented by the blue “start” rectangle in the above illustration) which it applies to every word.\n",
    "\n",
    "- After taking the dot product between the output embeddings and the ‘start’ weights, we apply the softmax activation to produce a probability distribution over all of the words. Whichever word has the highest probability of being the start token is the one that we pick.\n",
    "\n",
    "- We repeat this process for the end token–we have a separate weight vector this\n",
    "\n",
    "<img src=\"img/qa-bert-input-end.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LOAD FINE-TUNED BERT MODEL\n",
    "\n",
    "- We are going to use **<a href=\"https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad\">bert-large-uncased-whole-word-masking-finetuned-squad</a>**\n",
    "\n",
    "- BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way. It was trained with two objectives:\n",
    "    1. **Masked language modeling (MLM)**: taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence\n",
    "    2. **Next sentence prediction (NSP):** the models concatenates two masked sentences as inputs during pretraining. Sometimes they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to predict if the two sentences were following each other or not.\n",
    "    \n",
    " | Layers | Hidden State | Attention Heads | Parameters | Memory |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 12 | 1024 | 16 | 336M | 1.34GB |\n",
    "\n",
    "**Preprocessing**\n",
    "The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are then of the form:\n",
    "\n",
    "<i>[CLS] Sentence A [SEP] Sentence B [SEP]</i>\n",
    "\n",
    " - With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two \"sentences\" has a combined length of less than 512 tokens.\n",
    "\n",
    "- The details of the masking procedure for each sentence are the following:\n",
    "\n",
    "1. 15% of the tokens are masked.\n",
    "2. In 80% of the cases, the masked tokens are replaced by [MASK].\n",
    "3. In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n",
    "4. In the 10% remaining cases, the masked tokens are left as is.\n",
    "**Pretraining**\n",
    "- The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, β1=0.9 and β2=0.999, a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "#BASE_MODEL = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfeb2228c69e418d9130c229449e736f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108893186"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many parameters does BERT-large have?\"\n",
    "answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dict = tokenizer(question, answer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2129, 2116, 11709, 2515, 14324, 1011, 2312, 2031, 1029, 102, 14324, 1011, 2312, 2003, 2428, 2502, 1012, 1012, 1012, 2009, 2038, 2484, 1011, 9014, 1998, 2019, 7861, 8270, 4667, 2946, 1997, 1015, 1010, 6185, 2549, 1010, 2005, 1037, 2561, 1997, 16029, 2213, 11709, 999, 10462, 2009, 2003, 1015, 1012, 4090, 18259, 1010, 2061, 5987, 2009, 2000, 2202, 1037, 3232, 2781, 2000, 8816, 2000, 2115, 15270, 2497, 6013, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_dict['input_ids']\n",
    "token_type_ids = encoded_dict['token_type_ids']\n",
    "attention_mask = encoded_dict['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 70 input ids for the question + context\n"
     ]
    }
   ],
   "source": [
    "##Prepare input for the model \n",
    "input_ids = tokenizer.encode(question, answer_text)\n",
    "print(f\"There are {len(input_ids)} input ids for the question + context\")\n",
    "#Let´s convert \n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 >> [CLS]\n",
      "2129 >> how\n",
      "2116 >> many\n",
      "11709 >> parameters\n",
      "2515 >> does\n",
      "14324 >> bert\n",
      "1011 >> -\n",
      "2312 >> large\n",
      "2031 >> have\n",
      "1029 >> ?\n",
      "102 >> [SEP]\n",
      "14324 >> bert\n",
      "1011 >> -\n",
      "2312 >> large\n",
      "2003 >> is\n",
      "2428 >> really\n",
      "2502 >> big\n",
      "1012 >> .\n",
      "1012 >> .\n",
      "1012 >> .\n",
      "2009 >> it\n",
      "2038 >> has\n",
      "2484 >> 24\n",
      "1011 >> -\n",
      "9014 >> layers\n",
      "1998 >> and\n",
      "2019 >> an\n",
      "7861 >> em\n",
      "8270 >> ##bed\n",
      "4667 >> ##ding\n",
      "2946 >> size\n",
      "1997 >> of\n",
      "1015 >> 1\n",
      "1010 >> ,\n",
      "6185 >> 02\n",
      "2549 >> ##4\n",
      "1010 >> ,\n",
      "2005 >> for\n",
      "1037 >> a\n",
      "2561 >> total\n",
      "1997 >> of\n",
      "16029 >> 340\n",
      "2213 >> ##m\n",
      "11709 >> parameters\n",
      "999 >> !\n",
      "10462 >> altogether\n",
      "2009 >> it\n",
      "2003 >> is\n",
      "1015 >> 1\n",
      "1012 >> .\n",
      "4090 >> 34\n",
      "18259 >> ##gb\n",
      "1010 >> ,\n",
      "2061 >> so\n",
      "5987 >> expect\n",
      "2009 >> it\n",
      "2000 >> to\n",
      "2202 >> take\n",
      "1037 >> a\n",
      "3232 >> couple\n",
      "2781 >> minutes\n",
      "2000 >> to\n",
      "8816 >> download\n",
      "2000 >> to\n",
      "2115 >> your\n",
      "15270 >> cola\n",
      "2497 >> ##b\n",
      "6013 >> instance\n",
      "1012 >> .\n",
      "102 >> [SEP]\n"
     ]
    }
   ],
   "source": [
    "for input_id, token in zip(input_ids, tokens):\n",
    "    print(f\"{input_id} >> {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We’ve concatenated the question and answer_text together, but BERT still needs a way to distinguish them. BERT has two special “Segment” embeddings, one for segment “A” and one for segment “B”. Before the word embeddings go into the BERT layers, the segment A embedding needs to be added to the question tokens, and the segment B embedding needs to be added to each of the answer_text tokens.\n",
    "\n",
    "- These additions are handled for us by the transformer library, and all we need to do is specify a ‘0’ or ‘1’ for each token.\n",
    "\n",
    "- Note: In the transformers library, huggingface calls these token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.LongTensor([input_ids]),\n",
    "                torch.LongTensor([token_type_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = output.start_logits\n",
    "end_logits = output.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logits Tensor\n",
      "tensor([[ 0.3870,  0.3425,  0.2221, -0.1389, -0.0598,  0.0167,  0.2192, -0.0195,\n",
      "          0.0358,  0.1572,  0.2594,  0.0026,  0.3877,  0.2388, -0.2559, -0.2483,\n",
      "         -0.1877, -0.0095,  0.1381,  0.0192, -0.0996, -0.1723, -0.0579,  0.4340,\n",
      "          0.2965,  0.0091, -0.0136, -0.0008,  0.2370,  0.1961, -0.3182, -0.0174,\n",
      "         -0.0215, -0.2632,  0.3457, -0.4221,  0.2370,  0.0834,  0.0156, -0.4788,\n",
      "         -0.0394, -0.0963,  0.3294, -0.0429,  0.0606, -0.0020, -0.1711, -0.3001,\n",
      "         -0.4400, -0.3295, -0.2250, -0.2735,  0.2988, -0.1878, -0.0589, -0.0034,\n",
      "         -0.0199, -0.3540, -0.1299, -0.4777, -0.1572, -0.1968, -0.1163,  0.2619,\n",
      "         -0.1417, -0.2043, -0.0156, -0.2806,  0.3504,  0.2129]],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "\n",
      "\n",
      "Max value for start position 0.43402576446533203\n",
      "Max position value for start 23\n",
      "\n",
      "\n",
      "End Logits Tensor\n",
      "tensor([[ 0.2750,  0.3025,  0.1982,  0.2804,  0.0134,  0.1392, -0.3005,  0.1847,\n",
      "          0.2117,  0.1993, -0.2266, -0.0302, -0.1538,  0.0314, -0.2372, -0.0445,\n",
      "          0.2061,  0.1308,  0.0390,  0.2574,  0.2415, -0.1638,  0.1083, -0.0361,\n",
      "          0.0451, -0.1178,  0.0372, -0.3832, -0.1898,  0.1761,  0.0375,  0.0554,\n",
      "          0.2993, -0.0798, -0.2169,  0.2698, -0.4066,  0.2325,  0.1174,  0.3165,\n",
      "          0.3443,  0.3851,  0.1287,  0.2415,  0.2915, -0.0454,  0.0534, -0.2316,\n",
      "          0.0370, -0.2469,  0.0754, -0.5071, -0.3522,  0.1184,  0.0084,  0.0403,\n",
      "          0.1088, -0.4221, -0.1754,  0.2822, -0.1701,  0.0318,  0.0644,  0.2431,\n",
      "          0.3674, -0.1204,  0.1616,  0.2142, -0.2207, -0.2640]],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "\n",
      "\n",
      "Max value for start position 0.3851284384727478\n",
      "Max position value for start 41\n"
     ]
    }
   ],
   "source": [
    "print('Start logits Tensor')\n",
    "print(start_logits)\n",
    "print('\\n')\n",
    "print(f\"Max value for start position {start_logits.max()}\")\n",
    "print(f\"Max position value for start {start_logits.argmax()}\")\n",
    "print('\\n')\n",
    "print('End Logits Tensor')\n",
    "print(end_logits)\n",
    "print('\\n')\n",
    "print(f\"Max value for start position {end_logits.max()}\")\n",
    "print(f\"Max position value for start {end_logits.argmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
